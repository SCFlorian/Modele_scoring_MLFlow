{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f015f01",
   "metadata": {},
   "source": [
    "# Initiez-vous au MLOps (partie 1/2)\n",
    "## Option B - Exercice - Élaborez un modèle de scoring\n",
    "\n",
    "Vous êtes Data Scientist au sein d'une société financière, nommée \"Prêt à dépenser\", qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.\n",
    " \n",
    "L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un algorithme de classification en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.)\n",
    "\n",
    "La mission :\n",
    "- Construire et optimiser un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.\n",
    "- Analyser les features qui contribuent le plus au modèle, d’une manière générale (feature importance globale) et au niveau d’un client (feature importance locale), afin, dans un soucis de transparence, de permettre à un chargé d’études de mieux comprendre le score attribué par le modèle.\n",
    "- Mettre en œuvre une approche globale MLOps de bout en bout, du tracking des expérimentations à la pré-production du modèle.\n",
    "\n",
    "Mise en oeuvre des étapes orientées MLOps suivantes :\n",
    "- Dans le notebook d’entraînement des modèles, générer à l’aide de MLFlow un tracking d'expérimentations.\n",
    "- Lancer l’interface web “UI MLFlow\" d'affichage des résultats du tracking.\n",
    "- Réaliser avec MLFlow un stockage centralisé des modèles dans un “model registry”.\n",
    "- Tester le serving MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc87f8",
   "metadata": {},
   "source": [
    "### Objectif de cette étape :\n",
    "* Un ou plusieurs modèles entraînés, avec validation croisée et premières métriques d’évaluation.\n",
    "#### A faire :\n",
    "* Commencer par tester des modèles simples (Logistic Regression, Random Forest).\n",
    "* Comparer ensuite avec des modèles plus puissants (XGBoost, LightGBM).\n",
    "* UtiliserStratifiedKFoldpour conserver la distribution de classes et pour garantir une évaluation robuste.\n",
    "* Entraîner les modèles dans des notebooks clairs et documentés.\n",
    "* Stocker les scores et les hyperparamètres testés.\n",
    "#### Utilisation des métriques suivantes :\n",
    "* AUC-ROC\n",
    "* Recall sur la classe minoritaire\n",
    "* F1-score\n",
    "* Coût métier personnalisé (FN > FP)\n",
    "#### Réalisation : Même notebook que le notebook 3 mais en prenant comme scoring le coût métier dans le GridSearch\n",
    "**L'ensemble des étapes est suivi et enregistré sur un MLFLow en local**\n",
    "1) Séparation du jeu et création de la fonction de preprocessing et pipeline\n",
    "2) Mise en place de la recherche d'hyperparamètres via GridSearchCV\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "    * LightGBM\n",
    "    * Chaque modèle est enregistrée dans un run différent afin de comparer les scores de la métrique principale AUC\n",
    "    * Etant donné le long traitement des modèles, nous avons utilisé que class weight pour gérer les déséquilibres, \n",
    "    en appliquant \"balanced\" quand c'était possible sans chercher d'autres combinaisons de class weight.\n",
    "    * Dans la compréhension des résultats, mise en avant de l'importance de la métrique recall de la classe 1 afin de prendre le + possible de \"mauvais payeurs'.\n",
    "3) Validation croisée avec les meilleurs paramètres sur l'ensemble des modèles\n",
    "    * cross_validate couplé avec cross_validate_predict pour observer les résultats des métriques suivantes par modèle :\n",
    "        * roc_auc\n",
    "        * precision\n",
    "        * recall\n",
    "        * f1\n",
    "        * average_precision\n",
    "        * balanced_accuracy\n",
    "    * Calcul du seuil métier : avec un coût métier de l'ordre de 10 par FN et 1 par FP\n",
    "    * Graphique de la courbe ROC-AUC\n",
    "    * Graphique de comparaison du seuil métier versus le seuil de base (0.5)\n",
    "    * Enregistrement des résultats sous MLFlow dans un autre run mais dans une version 2 de chaque model dans model registry\n",
    "4) Version des résultats du modèle avec le seuil métier comme référence\n",
    "    * Enregistrement des résultats sous MLFlow dans un autre run mais dans une version 3 de chaque model dans model registry\n",
    "5) Comparaison du meilleur modèle sous MLFlow\n",
    "6) Vérification de cohérence avec les consignes, aucun des scores AUC ne dépassent 0.82, ils se situent entre 0.75 & 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41f836",
   "metadata": {},
   "source": [
    "### Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f38a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies de base\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librairies pour sauvegarde\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Ajout de MlFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "mlflow.set_tracking_uri(\"file://\" + os.path.abspath(\"../mlruns\"))\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Libraires scikit-learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV, cross_validate,cross_val_predict, StratifiedKFold)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score, auc, f1_score, balanced_accuracy_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Meilleure visualisation des grands df\n",
    "import itables.options as opt\n",
    "from itables import show\n",
    "\n",
    "# Activation de l’affichage interactif automatiquement dans les notebooks\n",
    "opt.warn_on_undocumented_option = False\n",
    "opt.notebook_connected = True\n",
    "opt.maxBytes = 0 # Pour ne pas tronquer les gros DataFrames\n",
    "opt.maxColumns = 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*does not have valid feature names.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce82a7",
   "metadata": {},
   "source": [
    "### Chargement du dataset fusionné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8765af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_modelisation = pd.read_csv(\"../data/processed/app_train_modelisation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33995b",
   "metadata": {},
   "source": [
    "#### Séparation de notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9b5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation de notre jeu fusionné en 2 grâce à la distinction - technique utilisée dans le kernel de Kaggle\n",
    "train_df = app_train_modelisation[app_train_modelisation['TARGET'].notnull()].copy() # jeu qui sera utilisé pour notre projet\n",
    "test_df = app_train_modelisation[app_train_modelisation['TARGET'].isnull()].copy()\n",
    "\n",
    "# Mise en place de nos X et y\n",
    "y = train_df['TARGET']\n",
    "X = train_df.drop(columns=['TARGET', 'is_train'], errors='ignore')\n",
    "X_test_kaggle = test_df.drop(columns=['TARGET', 'is_train'], errors='ignore') # pour Kaggle (pas utilisé ici)\n",
    "\n",
    "cols_encode = [col for col in X.columns if X[col].nunique() <= 2]\n",
    "cols_num = [col for col in X.columns if X[col].nunique()>2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fb124",
   "metadata": {},
   "source": [
    "#### On met en place notre suivi MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571b9ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/florianschorer/Documents/OpenClassrooms/Projets/OC_P6/Modele_scoring_MLFlow/mlruns/321322968827569333', creation_time=1762262527508, experiment_id='321322968827569333', last_update_time=1762262527508, lifecycle_stage='active', name='Tracking_models', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Tracking_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5a019",
   "metadata": {},
   "source": [
    "#### Notre jeu étant déjà séparé, on peut déjà créer notre fonction pour le preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed3cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessor(cols_num,cols_encode):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "        ('num', StandardScaler(),cols_num),\n",
    "        ('cat','passthrough',cols_encode)\n",
    "        ])\n",
    "    joblib.dump((preprocessor), \"../models/preprocessor.pkl\")\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d26144",
   "metadata": {},
   "source": [
    "#### Création de la fonction du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70832601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline(preprocessor,model):\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    joblib.dump((pipeline), \"../models/pipeline.pkl\")\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9b128",
   "metadata": {},
   "source": [
    "#### Création de la fonction du score métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a411275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return 10 * fn + fp\n",
    "business_cost_scorer = make_scorer(business_cost, greater_is_better=False) # Afin de miniser le coût"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590f543",
   "metadata": {},
   "source": [
    "#### Création de la fonction du GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcdfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearchcv(pipeline, param_grid, X, y):\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = pipeline,\n",
    "        param_grid = param_grid,\n",
    "        cv = cv,\n",
    "        n_jobs = -1,\n",
    "        scoring = business_cost_scorer,\n",
    "        verbose = 0\n",
    "    )\n",
    "    # Entraînement du modèle\n",
    "    grid_search.fit(X,y)\n",
    "    # J'enregistre quel est le meilleur modèle, je réalise la prédiction et j'évalue\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "    print(\"Meilleur_cout_metier\", grid_search.best_score_)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2312c",
   "metadata": {},
   "source": [
    "#### Modèle LogisticRegression\n",
    "* Vu la taille du fichier, on ne fait tourner que quelques hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a1428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'model__C': 0.5, 'model__class_weight': 'balanced', 'model__penalty': 'l2'}\n",
      "Meilleur_cout_metier -52688.333333333336\n"
     ]
    }
   ],
   "source": [
    "preprocessor = save_preprocessor(cols_num,cols_encode)\n",
    "model = LogisticRegression(max_iter=1000,random_state=42)\n",
    "pipeline = save_pipeline(preprocessor,model)\n",
    "param_grid = {\n",
    "    'model__penalty': ['l2'],'model__C': [0.5, 1.0, 2.0],\n",
    "    'model__class_weight': ['balanced']}\n",
    "grid_search = gridsearchcv(pipeline, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85686c",
   "metadata": {},
   "source": [
    "* Nous avons ici un score AUC correcte étant donné notre jeu déséquilibré.\n",
    "* On a utilisé ici le paramètre balanced afin gérér notre fort déséquilibre (92%-8%).\n",
    "* Le résultat peut s'expliquer par l'application d'une bonne partie du feature engineering du kernel de Kaggle.\n",
    "* Les meilleurs paramètres sont les suivants : **{'model__C': 0.5, 'model__class_weight': 'balanced', 'model__penalty': 'l2'}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ced928",
   "metadata": {},
   "source": [
    "#### Enregistrement des paramètres et du modèle de Logistic Regression sur MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f667fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(90807) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(90808) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/florianschorer/Library/Caches/pypoetry/virtualenvs/modele-scoring-mlflow-N-Hr5ii3-py3.13/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "python(90819) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(90826) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(90827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Successfully registered model 'LogisticRegression_GridSearch_cout_metier'.\n",
      "Created version '1' of model 'LogisticRegression_GridSearch_cout_metier'.\n"
     ]
    }
   ],
   "source": [
    "# Définition du clien pour gérer la partie model registry\n",
    "client = MlflowClient()\n",
    "projet_description = (\"Nous effectuons une recherche des hyperparamètres pour un modèle de LogisticRegression via GridSearchCV avec le coût métier.\")\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_GridSearch_cout_metier\",\n",
    "                      tags={\"Training Info\" : \"LogisticRegression_GridSearch_cout_metier\",\n",
    "                            \"mlflow.note.content\" : projet_description\n",
    "                            }):\n",
    "   # Meilleur modèle issu du GridSearch\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "   # Exemple du dataset, on retrouvera les colonnes et 3 lignes de données\n",
    "    X_sample = X.head(3).copy()\n",
    "    y_sample = best_model.predict(X.head(3))\n",
    "    signature = infer_signature(X.head(3), y_sample)\n",
    "    \n",
    "    # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Log du meilleur score du coût métier\n",
    "    mlflow.log_metric(\"Meilleur_cout_metier\", grid_search.best_score_)\n",
    "\n",
    "    # Log de tous les hyperparamètres testés\n",
    "    tested_params = grid_search.param_grid\n",
    "    flat_tested = {f\"tested_{k}\": str(v) for k, v in tested_params.items()}\n",
    "    mlflow.log_params(flat_tested)\n",
    "\n",
    "    # Log du pipeline complet dans MLflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, name=\"LogisticRegression_GridSearch_cout_metier\",\n",
    "        registered_model_name=\"LogisticRegression_GridSearch_cout_metier\",\n",
    "        input_example=X_sample,\n",
    "        signature=signature)\n",
    "    \n",
    "    client.set_model_version_tag(name=\"LogisticRegression_GridSearch_cout_metier\", version=1, key=\"LogisticRegression_GridSearch_cout_metier\", value=\"...\")\n",
    "    client.update_model_version(\n",
    "    name=\"LogisticRegression_GridSearch_cout_metier\",\n",
    "    version=1,\n",
    "    description=\"Recherche d'hyperparamètres via GridSearchCV pour LogisticRegression avec le coût métier\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad634507",
   "metadata": {},
   "source": [
    "#### Modèle Random Forest\n",
    "* Réutilisation des fonctions, on change juste le modèle et les paramètres testés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392848b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'model__class_weight': 'balanced', 'model__max_depth': 12, 'model__min_samples_leaf': 4, 'model__n_estimators': 200}\n",
      "Meilleur_cout_metier -57367.333333333336\n"
     ]
    }
   ],
   "source": [
    "preprocessor = save_preprocessor(cols_num,cols_encode)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "pipeline = save_pipeline(preprocessor,model)\n",
    "param_grid = {\n",
    "    'model__class_weight': ['balanced'],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [12],\n",
    "    'model__min_samples_leaf': [1, 4]}\n",
    "grid_search = gridsearchcv(pipeline, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56659acc",
   "metadata": {},
   "source": [
    "* Nous avons ici un score AUC correcte étant donné notre jeu déséquilibré.\n",
    "* On a utilisé ici le paramètre balanced afin gérér notre fort déséquilibre (92%-8%).\n",
    "* Le résultat peut s'expliquer par l'application d'une bonne partie du feature engineering du kernel de Kaggle.\n",
    "* Le score de ROC AUC est légèrement moins que notre baseline, le modèle de Logistic Regression.\n",
    "* Les meilleurs paramètres sont les suivants : **{'model__class_weight': 'balanced', 'model__max_depth': 12, 'model__min_samples_leaf': 4, 'model__n_estimators': 200}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d6c04",
   "metadata": {},
   "source": [
    "#### Enregistrement des paramètres et du modèle de Random Forest sur MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a28ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianschorer/Library/Caches/pypoetry/virtualenvs/modele-scoring-mlflow-N-Hr5ii3-py3.13/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'RandomForest_GridSearch_cout_metier'.\n",
      "Created version '1' of model 'RandomForest_GridSearch_cout_metier'.\n"
     ]
    }
   ],
   "source": [
    "# Définition du clien pour gérer la partie model registry\n",
    "client = MlflowClient()\n",
    "projet_description = (\"Nous effectuons une recherche des hyperparamètres pour un modèle de Random Forest via GridSearchCV avec le coût métier.\")\n",
    "with mlflow.start_run(run_name=\"RandomForest_GridSearch_cout_metier\",\n",
    "                      tags={\"Training Info\" : \"RandomForest_GridSearch_cout_metier\",\n",
    "                            \"mlflow.note.content\" : projet_description\n",
    "                            }):\n",
    "   # Meilleur modèle issu du GridSearch\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "   # Exemple du dataset, on retrouvera les colonnes et 3 lignes de données\n",
    "    X_sample = X.head(3).copy()\n",
    "    y_sample = best_model.predict(X.head(3))\n",
    "    signature = infer_signature(X.head(3), y_sample)\n",
    "    \n",
    "    # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Log du meilleur score moyen ROC AUC\n",
    "    mlflow.log_metric(\"Meilleur_cout_metier\", grid_search.best_score_)\n",
    "\n",
    "    # Log de tous les hyperparamètres testés\n",
    "    tested_params = grid_search.param_grid\n",
    "    flat_tested = {f\"tested_{k}\": str(v) for k, v in tested_params.items()}\n",
    "    mlflow.log_params(flat_tested)\n",
    "\n",
    "    # Log du pipeline complet dans MLflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, name=\"RandomForest_GridSearch_cout_metier\",\n",
    "        registered_model_name=\"RandomForest_GridSearch_cout_metier\",\n",
    "        input_example=X_sample,\n",
    "        signature=signature)\n",
    "    \n",
    "    client.set_model_version_tag(name=\"RandomForest_GridSearch_cout_metier\", version=1, key=\"RandomForest_GridSearch_cout_metier\", value=\"...\")\n",
    "    client.update_model_version(\n",
    "    name=\"RandomForest_GridSearch_cout_metier\",\n",
    "    version=1,\n",
    "    description=\"Recherche d'hyperparamètres via GridSearchCV pour RandomForest avec le coût métier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f81ec5",
   "metadata": {},
   "source": [
    "#### Modèle XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b07026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'model__learning_rate': 0.08, 'model__max_depth': 4, 'model__n_estimators': 300, 'model__scale_pos_weight': 12, 'model__subsample': 0.8}\n",
      "Meilleur_cout_metier -51034.333333333336\n"
     ]
    }
   ],
   "source": [
    "preprocessor = save_preprocessor(cols_num,cols_encode)\n",
    "model = XGBClassifier(eval_metric='logloss',random_state=42)\n",
    "pipeline = save_pipeline(preprocessor,model)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 300],            \n",
    "    'model__max_depth': [3, 4, 6],              \n",
    "    'model__learning_rate': [0.05, 0.08],    \n",
    "    'model__subsample': [0.8],               \n",
    "    'model__scale_pos_weight': [12, 14]       \n",
    "    }\n",
    "grid_search = gridsearchcv(pipeline, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cdaa4",
   "metadata": {},
   "source": [
    "* Nous avons ici un score AUC correcte étant donné notre jeu déséquilibré.\n",
    "* On a utilisé ici le paramètre balanced afin gérér notre fort déséquilibre (92%-8%).\n",
    "* Le résultat peut s'expliquer par l'application d'une bonne partie du feature engineering du kernel de Kaggle.\n",
    "* Le score de ROC AUC est meilleur que les deux modèles précédents.\n",
    "* Les meilleurs paramètres sont les suivants : **{'model__learning_rate': 0.08, 'model__max_depth': 4, 'model__n_estimators': 300, 'model__scale_pos_weight': 12, 'model__subsample': 0.8}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1d60f",
   "metadata": {},
   "source": [
    "#### Enregistrement des paramètres et du modèle de XGBoost sur MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e95db97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianschorer/Library/Caches/pypoetry/virtualenvs/modele-scoring-mlflow-N-Hr5ii3-py3.13/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'XGBoost_GridSearch_cout_metier'.\n",
      "Created version '1' of model 'XGBoost_GridSearch_cout_metier'.\n"
     ]
    }
   ],
   "source": [
    "# Définition du clien pour gérer la partie model registry\n",
    "client = MlflowClient()\n",
    "projet_description = (\"Nous effectuons une recherche des hyperparamètres pour un modèle de XGBoost via GridSearchCV avec le coût métier.\")\n",
    "with mlflow.start_run(run_name=\"XGBoost_GridSearch_cout_metier\",\n",
    "                      tags={\"Training Info\" : \"XGBoost_GridSearch_cout_metier\",\n",
    "                            \"mlflow.note.content\" : projet_description\n",
    "                            }):\n",
    "   # Meilleur modèle issu du GridSearch\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "   # Exemple du dataset, on retrouvera les colonnes et 3 lignes de données\n",
    "    X_sample = X.head(3).copy()\n",
    "    y_sample = best_model.predict(X.head(3))\n",
    "    signature = infer_signature(X.head(3), y_sample)\n",
    "    \n",
    "    # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Log du meilleur score moyen ROC AUC\n",
    "    mlflow.log_metric(\"Meilleur_cout_metier\", grid_search.best_score_)\n",
    "\n",
    "    # Log de tous les hyperparamètres testés\n",
    "    tested_params = grid_search.param_grid\n",
    "    flat_tested = {f\"tested_{k}\": str(v) for k, v in tested_params.items()}\n",
    "    mlflow.log_params(flat_tested)\n",
    "\n",
    "    # Log du pipeline complet dans MLflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, name=\"XGBoost_GridSearch_cout_metier\",\n",
    "        registered_model_name=\"XGBoost_GridSearch_cout_metier\",\n",
    "        input_example=X_sample,\n",
    "        signature=signature)\n",
    "    \n",
    "    client.set_model_version_tag(name=\"XGBoost_GridSearch_cout_metier\", version=1, key=\"XGBoost_GridSearch_cout_metier\", value=\"...\")\n",
    "    client.update_model_version(\n",
    "    name=\"XGBoost_GridSearch_cout_metier\",\n",
    "    version=1,\n",
    "    description=\"Recherche d'hyperparamètres via GridSearchCV pour XGBoost avec le coût métier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e020e",
   "metadata": {},
   "source": [
    "#### Modèle LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f0d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'model__bagging_fraction': 0.8, 'model__feature_fraction': 0.8, 'model__force_row_wise': True, 'model__learning_rate': 0.05, 'model__n_estimators': 300, 'model__num_leaves': 60, 'model__scale_pos_weight': 12, 'model__verbosity': -1}\n",
      "Meilleur_cout_metier -50906.333333333336\n"
     ]
    }
   ],
   "source": [
    "preprocessor = save_preprocessor(cols_num,cols_encode)\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "pipeline = save_pipeline(preprocessor,model)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 300],\n",
    "    'model__num_leaves': [30, 60],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__feature_fraction': [0.8],\n",
    "    'model__bagging_fraction': [0.8],\n",
    "    'model__scale_pos_weight': [12, 16],\n",
    "    'model__verbosity': [-1],\n",
    "    'model__force_row_wise': [True]}\n",
    "grid_search = gridsearchcv(pipeline, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4e2fc",
   "metadata": {},
   "source": [
    "* Nous avons ici un score AUC correcte étant donné notre jeu déséquilibré.\n",
    "* On a utilisé ici le paramètre balanced afin gérér notre fort déséquilibre (92%-8%).\n",
    "* Le résultat peut s'expliquer par l'application d'une bonne partie du feature engineering du kernel de Kaggle.\n",
    "* Le score de ROC AUC est meilleur que les trois modèles précédents, il surclasse de très peu le modèle d'XGBoost.\n",
    "* Les meilleurs paramètres sont les suivants : **{'model__bagging_fraction': 0.8, 'model__feature_fraction': 0.8, 'model__learning_rate': 0.05, 'model__n_estimators': 300, 'model__num_leaves': 60, 'model__scale_pos_weight': 12}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385f5db",
   "metadata": {},
   "source": [
    "#### Enregistrement des paramètres et du modèle de LightGBM sur MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a277bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianschorer/Library/Caches/pypoetry/virtualenvs/modele-scoring-mlflow-N-Hr5ii3-py3.13/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'LightGBM_GridSearch_cout_metier'.\n",
      "Created version '1' of model 'LightGBM_GridSearch_cout_metier'.\n"
     ]
    }
   ],
   "source": [
    "# Définition du clien pour gérer la partie model registry\n",
    "client = MlflowClient()\n",
    "projet_description = (\"Nous effectuons une recherche des hyperparamètres pour un modèle de LightGBM via GridSearchCV avec le coût métier.\")\n",
    "with mlflow.start_run(run_name=\"LightGBM_GridSearch_cout_metier\",\n",
    "                      tags={\"Training Info\" : \"LightGBM_GridSearch_cout_metier\",\n",
    "                            \"mlflow.note.content\" : projet_description\n",
    "                            }):\n",
    "   # Meilleur modèle issu du GridSearch\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "   # Exemple du dataset, on retrouvera les colonnes et 3 lignes de données\n",
    "    X_sample = X.head(3).copy()\n",
    "    y_sample = best_model.predict(X.head(3))\n",
    "    signature = infer_signature(X.head(3), y_sample)\n",
    "    \n",
    "    # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Log du meilleur score moyen ROC AUC\n",
    "    mlflow.log_metric(\"Meilleur_cout_metier\", grid_search.best_score_)\n",
    "\n",
    "    # Log de tous les hyperparamètres testés\n",
    "    tested_params = grid_search.param_grid\n",
    "    flat_tested = {f\"tested_{k}\": str(v) for k, v in tested_params.items()}\n",
    "    mlflow.log_params(flat_tested)\n",
    "\n",
    "    # Log du pipeline complet dans MLflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, name=\"LightGBM_GridSearch_cout_metier\",\n",
    "        registered_model_name=\"LightGBM_GridSearch_cout_metier\",\n",
    "        input_example=X_sample,\n",
    "        signature=signature)\n",
    "    \n",
    "    client.set_model_version_tag(name=\"LightGBM_GridSearch_cout_metier\", version=1, key=\"LightGBM_GridSearch_cout_metier\", value=\"...\")\n",
    "    client.update_model_version(\n",
    "    name=\"LightGBM_GridSearch_cout_metier\",\n",
    "    version=1,\n",
    "    description=\"Recherche d'hyperparamètres via GridSearchCV pour LightGBM avec le coût métier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a8c32",
   "metadata": {},
   "source": [
    "#### Nous n'effectuons pas plus d'analyse ici car causé par le nombre d'hyperparamètres limités, les meilleurs paramètres, même en optimisant selon le coût métier ne change pas les hyperparamètres déjà en place dans le précédent notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modele-scoring-mlflow-N-Hr5ii3-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
